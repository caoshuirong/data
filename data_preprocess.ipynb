{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6177c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2905dc4",
   "metadata": {},
   "source": [
    "## 1.1 生成数据集 （修改window_len即可）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90ac770a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "987e7ab8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "735"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('train.csv',header = None)\n",
    "\n",
    "# data = data.to_numpy()\n",
    "total_num = data.shape[0]\n",
    "total_len = data.shape[1]\n",
    "total_num\n",
    "total_len\n",
    "\n",
    "output_size = 56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "608fdf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_min 归一化\n",
    "def max_min(x,total_num):\n",
    "    min_val = x.values.min(axis=1)\n",
    "    max_val = x.values.max(axis=1)\n",
    "    values = (x.values-min_val.reshape(total_num,1))/(max_val - min_val).reshape(total_num,1)\n",
    "    return values,min_val,max_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a437ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最后生成测试数据时进行还原，然后与测试的label进行比较\n",
    "def max_min_reverse(x_numpy,min_val,max_val):\n",
    "    values = (x_numpy + min_val.reshape(total_num,1)) * (max_val - min_val).reshape(total_num,1)\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2ab033d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# max_min 归一化\n",
    "data,min_val,max_val = max_min(data,total_num)\n",
    "\n",
    "# data = max_min_reverse(data,min_val,max_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3921798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.17652547, 0.19387946, 0.27075947, 0.45698824, 0.35062512,\n",
       "       0.21869752, 0.20171674, 0.15282702, 0.26180258, 0.31293152,\n",
       "       0.44803135, 0.44485912, 0.24034335, 0.25527151, 0.22728121,\n",
       "       0.31349132, 0.47564844, 0.44243329, 0.42955775, 0.09852584,\n",
       "       0.18950908, 0.13342041, 0.2358649 , 0.29128569, 0.43758164,\n",
       "       0.43571562, 0.26348199, 0.22560179, 0.18660198, 0.24706102,\n",
       "       0.26030976, 0.46725135, 0.43814144, 0.22261616, 0.1225975 ,\n",
       "       0.04217205, 0.29427132, 0.23885053, 0.32300802, 0.09740623,\n",
       "       0.15898489, 0.16626236, 0.15431984, 0.29837656, 0.29632394,\n",
       "       0.45101698, 0.42974436, 0.15898489, 0.64582945, 0.21888412,\n",
       "       0.26982646, 0.3021086 , 0.43161038, 0.09106177, 0.15898489,\n",
       "       0.45381601, 0.1977981 , 0.25639112, 0.29688375, 0.46575854,\n",
       "       0.37339056, 0.24818063, 0.20190334, 0.17465945, 0.2362381 ,\n",
       "       0.30901288, 0.1599179 , 0.27729054, 0.20899422, 0.22560179,\n",
       "       0.23325247, 0.27075947, 0.3493189 , 0.44915096, 0.37824221,\n",
       "       0.25676432, 0.22522859, 0.21627169, 0.25601791, 0.30919948,\n",
       "       0.47620825, 0.37842881, 0.25844374, 0.25321888, 0.16775518,\n",
       "       0.26049636, 0.32356783, 0.4814331 , 0.36070162, 0.07333458,\n",
       "       0.09348759, 0.26478821, 0.26068296, 0.31386453, 0.48927039,\n",
       "       0.4056727 , 0.23157305, 0.22280276, 0.22168315, 0.23082665,\n",
       "       0.28960627, 0.42713193, 0.40455309, 0.28718044, 0.24258257,\n",
       "       0.2548983 , 0.24426199, 0.29688375, 0.51949991, 0.46314611,\n",
       "       0.23717111, 0.25695092, 0.20563538, 0.27206568, 0.34838589,\n",
       "       0.49150961, 0.40847173, 0.27896996, 0.19929091, 0.23978354,\n",
       "       0.31386453, 0.37786901, 0.53722709, 0.39764882, 0.27561112,\n",
       "       0.26833364, 0.2166449 , 0.29296511, 0.36294085, 0.47061019,\n",
       "       0.38794551, 0.26497481, 0.25060646, 0.22746781, 0.33252472,\n",
       "       0.32244822, 0.52472476, 0.43534241, 0.28102258, 0.28531442,\n",
       "       0.22093674, 0.30490763, 0.36443366, 0.54301176, 0.35473036,\n",
       "       0.28923307, 0.25060646, 0.19891771, 0.34596007, 0.34987871,\n",
       "       0.52957641, 0.4623997 , 0.28456802, 0.27467811, 0.22019033,\n",
       "       0.30098899, 0.36480687, 0.48628475, 0.42993096, 0.28550103,\n",
       "       0.        , 0.19499907, 0.29800336, 0.37973503, 0.54413137,\n",
       "       0.46557194, 0.29053928, 0.30919948, 0.26105617, 0.28662064,\n",
       "       0.33737638, 0.52267214, 0.40641911, 0.28997947, 0.27262549,\n",
       "       0.1793245 , 0.26908005, 0.38234745, 0.50681097, 0.1128942 ,\n",
       "       0.55084904, 0.29949617, 0.24090315, 0.35752939, 0.34204143,\n",
       "       0.49822728, 0.4340362 , 0.29576414, 0.21459227, 0.1888412 ,\n",
       "       0.29557753, 0.33252472, 0.52323195, 0.44485912, 0.27896996,\n",
       "       0.25415189, 0.23829073, 0.28046277, 0.32580705, 0.49225602,\n",
       "       0.44467251, 0.29874977, 0.2453816 , 0.21067363, 0.27169248,\n",
       "       0.35043851, 0.45400261, 0.40324687, 0.26945326, 0.27299869,\n",
       "       0.19891771, 0.26963986, 0.35771599, 0.52584437, 0.46463893,\n",
       "       0.28885986, 0.26366859, 0.22186975, 0.32953909, 0.36741929,\n",
       "       0.54151894, 0.45605523, 0.29725695, 0.27467811, 0.23455869,\n",
       "       0.31796977, 0.32375443, 0.44000746, 0.43123717, 0.2925919 ,\n",
       "       0.25919015, 0.21179325, 0.28475462, 0.34260123, 0.49393544,\n",
       "       0.42507931, 0.29147229, 0.23157305, 0.16644896, 0.29165889,\n",
       "       0.32580705, 0.52771039, 0.48889718, 0.31629035, 0.25023325,\n",
       "       0.12912857, 0.22056354, 0.31405113, 0.5189401 , 0.43944766,\n",
       "       0.11140138, 0.13864527, 0.13435342, 0.321142  , 0.32841948,\n",
       "       0.35062512, 0.30472103, 0.15898489, 0.59973876, 0.22466878,\n",
       "       0.26758724, 0.34651987, 0.5185669 , 0.37824221, 0.32002239,\n",
       "       0.29669714, 0.25639112, 0.35976861, 0.35958201, 0.60626983,\n",
       "       0.50867699, 0.31666356, 0.32020899, 0.29184549, 0.38384027,\n",
       "       0.42545251, 0.60869565, 0.45941407, 0.        , 0.5280836 ,\n",
       "       0.5941407 , 0.07240157, 0.24691771, 0.24146296, 0.05784661,\n",
       "       0.24687442, 0.26348199, 0.28139578, 0.1035641 , 0.24426199,\n",
       "       0.32375443, 0.32151521, 0.        , 0.14032469, 0.20470237,\n",
       "       0.19425266, 0.29725695, 0.39428998, 0.39932823, 0.25788393,\n",
       "       0.24332898, 0.1698078 , 0.2925919 , 0.3110655 , 0.48815077,\n",
       "       0.3870125 , 0.        , 0.13043478, 0.1602911 , 0.22728121,\n",
       "       0.3392424 , 0.45810786, 0.45642844, 0.29445792, 0.23885053,\n",
       "       0.20041052, 0.27225229, 0.32319463, 0.49318903, 0.47434223,\n",
       "       0.13267401, 0.36704609, 0.20507557, 0.25956335, 0.32711327,\n",
       "       0.49878709, 0.12147789, 0.33737638, 0.63034148, 0.19331965,\n",
       "       0.24314238, 0.35510356, 0.55961933, 0.40623251, 0.30957268,\n",
       "       0.19518567, 0.18193693, 0.22466878, 0.34166822, 0.52771039,\n",
       "       0.42078746, 0.26366859, 0.26236238, 0.1979847 , 0.2450084 ,\n",
       "       0.37674939, 0.56782982, 0.38775891, 0.28755365, 0.1796977 ,\n",
       "       0.09665982, 0.31666356, 0.38757231, 0.5661504 , 0.46277291,\n",
       "       0.14797537, 0.41630901, 0.22858742, 0.22746781, 0.37152454,\n",
       "       0.50587796, 0.47228961, 0.28046277, 0.24594141, 0.18175033,\n",
       "       0.2455682 , 0.40063445, 0.49374883, 0.43795484, 0.27915656,\n",
       "       0.28643404, 0.26796044, 0.3584624 , 0.59134167, 0.48479194,\n",
       "       0.48796417, 0.24426199, 0.34577347, 0.36443366, 0.31666356,\n",
       "       0.3302855 , 0.60906886, 0.47061019, 0.3015488 , 0.27057287,\n",
       "       0.22560179, 0.25825714, 0.4344094 , 0.44672514, 0.47620825,\n",
       "       0.31778317, 0.26198918, 0.26889345, 0.29949617, 0.47546184,\n",
       "       0.59637992, 0.46818436, 0.33233812, 0.23959694, 0.22130995,\n",
       "       0.30994589, 0.38066804, 0.566337  , 0.41780183, 0.31946259,\n",
       "       0.28699384, 0.1977981 , 0.3582758 , 0.24691771, 0.60440381,\n",
       "       0.48199291, 0.31255831, 0.23735772, 0.28400821, 0.20974062,\n",
       "       0.31330472, 0.63631274, 0.44094047, 0.3015488 , 0.24276917,\n",
       "       0.23008024, 0.30528084, 0.40679231, 0.58331778, 0.47434223,\n",
       "       0.29389812, 0.26254898, 0.19611868, 0.34726628, 0.51987311,\n",
       "       0.5851838 , 0.5097966 , 0.30360142, 0.31685016, 0.28027617,\n",
       "       0.35323754, 0.41686882, 0.57230827, 0.49710767, 0.30789326,\n",
       "       0.32543385, 0.25993656, 0.29800336, 0.4056727 , 0.56652361,\n",
       "       0.45064378, 0.35547677, 0.34036201, 0.21907072, 0.33662997,\n",
       "       0.43179698, 0.61653294, 0.31349132, 0.15898489, 0.14461653,\n",
       "       0.2924053 , 0.40772532, 0.49860049, 0.63967158, 0.31349132,\n",
       "       0.15898489, 0.31890278, 0.20451577, 0.32580705, 0.43776824,\n",
       "       0.6699011 , 0.51558127, 0.3489457 , 0.35622318, 0.29781676,\n",
       "       0.30098899, 0.45829446, 0.61168128, 0.49337563, 0.43664863,\n",
       "       0.33439074, 0.25396529, 0.35976861, 0.42209367, 0.58369099,\n",
       "       0.42321329, 0.15898489, 0.1033775 , 0.17401984, 0.23847733,\n",
       "       0.46874417, 0.67120731, 0.52229894, 0.36424706, 0.35267774,\n",
       "       0.24314238, 0.40828513, 0.50401194, 0.56428438, 0.47938048,\n",
       "       0.30882627, 0.34558686, 0.26478821, 0.37562978, 0.43272999,\n",
       "       0.64750886, 0.5943273 , 0.3491323 , 0.33662997, 0.32375443,\n",
       "       0.37077813, 0.49748087, 0.74752752, 0.49804068, 0.3493189 ,\n",
       "       0.32599366, 0.28979287, 0.38999813, 0.49150961, 0.71095354,\n",
       "       0.53480127, 0.40100765, 0.38234745, 0.30826647, 0.27542452,\n",
       "       0.49356223, 0.75181937, 0.52938981, 0.37171114, 0.36014182,\n",
       "       0.40436649, 0.36331405, 0.4620265 , 0.66131741, 0.58089196,\n",
       "       0.37283075, 0.37637619, 0.30602724, 0.3207688 , 0.4814331 ,\n",
       "       0.79958948, 0.43291659, 0.39988804, 0.37973503, 0.2830752 ,\n",
       "       0.35081172, 0.50102631, 0.66430304, 0.48740437, 0.35118492,\n",
       "       0.35155813, 0.22522859, 0.33607016, 0.49225602, 0.69453256,\n",
       "       0.52285874, 0.4526964 , 0.35958201, 0.2638552 , 0.28867326,\n",
       "       0.42899795, 0.70927412, 0.5754805 , 0.3871991 , 0.30994589,\n",
       "       0.31479754, 0.3489457 , 0.53648069, 0.76170927, 0.55756671,\n",
       "       0.415376  , 0.2735585 , 0.21440567, 0.34726628, 0.49785408,\n",
       "       0.65012129, 0.47303601, 0.36629968, 0.30845307, 0.25191267,\n",
       "       0.36368726, 0.44541892, 0.65833178, 0.20806121, 0.47788767,\n",
       "       0.33271133, 0.24370218, 0.42246688, 0.51614107, 0.7451017 ,\n",
       "       0.08098526, 0.15898489, 0.        , 0.21011383, 0.41910804,\n",
       "       0.51464826, 0.7829819 , 0.57212166, 0.39055794, 0.35267774,\n",
       "       0.28699384, 0.39391678, 0.50438515, 0.7268147 , 0.56036574,\n",
       "       0.38607949, 0.32506065, 0.24575481, 0.36872551, 0.57771972,\n",
       "       0.69490577, 0.31349132, 0.40679231, 0.33177832, 0.2261616 ,\n",
       "       0.07725322, 0.71916402, 0.72364247, 0.60328419, 0.13286061,\n",
       "       0.25527151, 0.29707035, 0.42862474, 0.55364807, 0.80481433,\n",
       "       0.64359022, 0.10151148, 0.26068296, 0.2547117 , 0.39746221,\n",
       "       0.56055234, 0.81059899, 0.64526964, 0.49039   , 0.30565404,\n",
       "       0.29781676, 0.40753872, 0.50494495, 0.78876656, 0.61056167,\n",
       "       0.4525098 , 0.35062512, 0.3306587 , 0.40959134, 0.62362381,\n",
       "       0.93319649, 0.77047957, 0.53181564, 0.6411644 , 0.68949431,\n",
       "       1.        , 0.18622877, 0.14536294, 0.33233812, 0.25172607,\n",
       "       0.37861541, 0.38290726, 0.71244635, 0.19761149, 0.45456242,\n",
       "       0.37842881, 0.32188841, 0.30453443, 0.23474529, 0.31703676,\n",
       "       0.4340362 , 0.6700877 , 0.58369099, 0.38757231, 0.31330472,\n",
       "       0.25135286, 0.33271133, 0.46072028, 0.69546557, 0.62045158,\n",
       "       0.15898489, 0.39410338, 0.30640045, 0.39111775, 0.45810786,\n",
       "       0.80668035, 0.56260496, 0.40996455, 0.34054861, 0.28867326,\n",
       "       0.42769173, 0.57454749, 0.792312  , 0.55010263, 0.45083038,\n",
       "       0.31890278, 0.28176899, 0.37842881, 0.47658145, 0.7924986 ,\n",
       "       0.55570069, 0.32002239, 0.39279716, 0.26572122, 0.37898862,\n",
       "       0.54151894, 0.77234559, 0.65609255, 0.4241463 , 0.34633327,\n",
       "       0.31461093, 0.44859116, 0.54263855, 0.76040306, 0.55588729,\n",
       "       0.40735212, 0.34054861, 0.27952976, 0.39149095, 0.56447098,\n",
       "       0.82254152, 0.59899235, 0.44094047, 0.2450084 , 0.3203956 ,\n",
       "       0.35454376, 0.53610748, 0.71804441, 0.6131741 , 0.13957828,\n",
       "       0.05430118, 0.1599179 , 0.40380668, 0.50867699, 0.7921254 ,\n",
       "       0.62101138, 0.48927039, 0.32897929, 0.25937675, 0.42582571,\n",
       "       0.396156  , 0.71281956, 0.70442247, 0.52267214, 0.39111775])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69d3fc28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(56,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 划分训练集和验证集\n",
    "\n",
    "window_len = 56\n",
    "\n",
    "train_series = data[:,0:total_len-output_size]\n",
    "train_size = train_series.shape[1]\n",
    "\n",
    "y_val_list = [data[i,total_len-output_size:] for i in range(total_num)]\n",
    "y_val_list[0].shape\n",
    "\n",
    "x_test_list = [data[i,total_len-window_len:] for i in range(total_num)]\n",
    "x_test_list[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ff7ebc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo 滑动窗口截取\n",
    "# 每个样本的shape为(111,window_len) (111,window_len:window_len+56)\n",
    "\n",
    "\n",
    "x_train_list = []\n",
    "y_train_list = []\n",
    "x_val_list = []\n",
    "for i in range(total_num):\n",
    "    for j in range(0,train_size - window_len-output_size,20):\n",
    "        x_train = train_series[i,j:j+window_len]\n",
    "        y_train = train_series[i,j+window_len:j+window_len+output_size]\n",
    "        x_train_list.append(x_train)\n",
    "        y_train_list.append(y_train)\n",
    "\n",
    "\n",
    "x_val_list = [train_series[i,train_size-output_size-window_len:] for i in range(total_num)]\n",
    "\n",
    "dataset ={\n",
    "    'x_train':x_train_list,\n",
    "    'y_train':y_train_list,\n",
    "    'x_val':x_val_list,\n",
    "    'y_val':y_val_list,\n",
    "    'x_test':x_test_list,\n",
    "    'max_val':max_val,\n",
    "    'min_val':min_val\n",
    "}\n",
    "\n",
    "# todo 问题在于 700多维的数据，LSTM记不住该如何划分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "690d70a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('dataset_%d.pkl'%window_len,'wb') as f:\n",
    "    pickle.dump(dataset,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea2219c",
   "metadata": {},
   "source": [
    "## 1.2 封装数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e30c59cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.functional as f\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef537683",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self,x_train,y_train):\n",
    "        super(Dataset,self).__init__()\n",
    "        self.length = len(x_train)\n",
    "        self.data = [torch.Tensor(x_train[i]) for i in range(self.length)]\n",
    "        self.label = [torch.Tensor(y_train[i]) for i in range(self.length)]\n",
    "        \n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        return self.data[index],self.label[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3415033f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 封装训练和验证集\n",
    "x_train = dataset['x_train']\n",
    "y_train = dataset['y_train']\n",
    "x_val = dataset['x_val']\n",
    "y_val = dataset['y_val']\n",
    "min_val = dataset['min_val']\n",
    "max_val = dataset['max_val']\n",
    "    \n",
    "\n",
    "train_dataset = MyDataset(x_train,y_train)\n",
    "val_dataset = MyDataset(x_val,y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset,batch_size = 32,shuffle = True)\n",
    "val_loader = DataLoader(val_dataset,batch_size = total_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c041198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 封装测试集\n",
    "x_test = dataset['x_test']\n",
    "y_test = pd.read_csv('test.csv',header = None)\n",
    "y_test = [y_test.iloc[i,:].to_numpy() for i in range(len(x_test))]\n",
    "test_dataset = MyDataset(x_test,y_test)\n",
    "test_loader = DataLoader(test_dataset,batch_size = total_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb05015b",
   "metadata": {},
   "source": [
    "# 3.搭建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f9e01a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(level = logging.INFO)\n",
    "handler = logging.FileHandler(\"log.txt\")\n",
    "handler.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6bfeee71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "class LossFunction(Enum):\n",
    "    MAE = 1\n",
    "    MSE = 2\n",
    "    SmoothL1 = 3\n",
    "\n",
    "class Mode(Enum):\n",
    "    Trian = 1\n",
    "    Valid = 2\n",
    "    Test = 3\n",
    "\n",
    "class Optim(Enum):\n",
    "    SGD = 1\n",
    "    Adam = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a1829c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_weights(model):\n",
    "    for m in model.modules():\n",
    "#         if isinstance(m, nn.LSTM):\n",
    "#             [nn.init.orthogonal_(para) for name,para in m.name_parameters() if 'weight' in name]\n",
    "                \n",
    "        if isinstance(m,nn.Linear):\n",
    "            nn.init.kaiming_normal_(m.weight.data)\n",
    "    \n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,input_size,rnn_inpt_size,rnn_hidden_size,dropout_rate=0.2):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.rnn_input_size = rnn_inpt_size\n",
    "        self.rnn_hidden_size = rnn_hidden_size\n",
    "        \n",
    "        self.linear = nn.Linear(input_size,rnn_inpt_size)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.lstm = nn.LSTM(rnn_inpt_size,rnn_hidden_size,batch_first=True)\n",
    "        self.apply(initial_weights)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        emb = self.linear(x) # batch size * seq_len\n",
    "        _,(h_n,_) = self.lstm(emb.reshape(x.shape[0],1,self.rnn_input_size))\n",
    "        \n",
    "        return h_n\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc13119e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,ouput_size,rnn_inpt_size,rnn_hidden_size,dropout_rate=0.2):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.rnn_input_size = rnn_inpt_size\n",
    "        self.rnn_hidden_size = rnn_hidden_size\n",
    "        \n",
    "        self.linear = nn.Linear(rnn_hidden_size,ouput_size)\n",
    "#         self.dropout = nn.Dropout(dropout_rate)\n",
    "#         self.lstm = nn.LSTM(rnn_inpt_size,rnn_hidden_size,batch_first=True)\n",
    "#         self.apply(initial_weights)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        emb = self.linear(x.reshape(-1,self.rnn_hidden_size)) # batch size * seq_len\n",
    "        \n",
    "        return emb\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4501f2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self,input_size,ouput_size,rnn_inpt_size,rnn_hidden_size,loss_type=LossFunction.MAE,dropout_rate=0.2):\n",
    "        super(Seq2Seq,self).__init__()\n",
    "        self.name = 'LSTM-based seq-to-seq'\n",
    "        self.loss_type = loss_type\n",
    "        self.encoder = Encoder(input_size,rnn_inpt_size,rnn_hidden_size,dropout_rate)\n",
    "        self.decoder = Decoder(ouput_size,rnn_hidden_size,rnn_hidden_size,dropout_rate)\n",
    "#         self.min = min_val\n",
    "#         self.max = max_val\n",
    "        if loss_type == LossFunction.MAE:\n",
    "            self.loss_fun = nn.L1Loss()\n",
    "        elif loss_type == LossFunction.MSE:\n",
    "            self.loss_fun = nn.MSELoss()\n",
    "        elif loss_type == LossFunction.SmoothL1:\n",
    "            self.loss_fun = nn.SmoothL1Loss()\n",
    "        else:\n",
    "            raise ValueError(\"please check loss_fun type!\")\n",
    "        \n",
    "    def forward(self,x,label=None):\n",
    "        out = self.encoder(x)\n",
    "        out = self.decoder(out)\n",
    "#         if mode == Mode.Test:\n",
    "#             out = max_min_reverse(out.cpu().detach().numpy(),self.min,self.max)\n",
    "        if label is not None:\n",
    "            loss = self.loss_fun(out,label)\n",
    "            out = (out,loss)\n",
    "            \n",
    "        return  out\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b2091f",
   "metadata": {},
   "source": [
    "# 4.训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dbad709f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6917f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape(y_true, y_pred):\n",
    "    return 2.0 * np.mean(np.abs(y_pred - y_true) / (np.abs(y_pred) + np.abs(y_true))) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4bc15cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Trainer:\n",
    "    def __init__(self,model,train_loader,val_loader,test_loader,min_val,max_val,train_args):\n",
    "        super(Trainer,self).__init__()\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.lr = train_args['lr']\n",
    "        self.batch_size = train_args['batch_size']\n",
    "        self.optim_type = train_args['optim']\n",
    "        self.epochs = train_args['epochs']\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
    "        self.model.to(self.device)\n",
    "        self.min_loss = 1e8\n",
    "        self.best_model = None\n",
    "        self.normalize_min_val = min_val\n",
    "        self.mormalize_max_val = max_val\n",
    "        if self.optim_type == Optim.SGD:\n",
    "            self.optim = optim.SGD(self.model.parameters(),lr = self.lr)\n",
    "        elif self.optim_type == Optim.Adam:\n",
    "            self.optim = optim.Adam(self.model.parameters(),lr = self.lr)\n",
    "        else:\n",
    "            raise ValueError(\"Not a recognized optimizer\")\n",
    "    \n",
    "    def __train_one_peoch(self):\n",
    "        processbar = tqdm(range(self.train_loader.__len__()))\n",
    "        self.model.train()\n",
    "        train_loss = []\n",
    "        train_smape = []\n",
    "        for idx,(x_batch,y_batch) in enumerate(train_loader):\n",
    "            x_batch = x_batch.to(self.device)\n",
    "            y_batch = y_batch.to(self.device)\n",
    "            pred,loss = self.model(x_batch,y_batch)\n",
    "            self.optim.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optim.step()\n",
    "            train_loss.append(loss.item())\n",
    "            train_smape.append(SMAPE(pred.reshape(-1),y_batch))\n",
    "            processbar.update(1)\n",
    "        \n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = []\n",
    "            val_smape = []\n",
    "            for idx,(x_batch,y_batch) in enumerate(val_loader):\n",
    "                x_batch = x_batch.to(self.device)\n",
    "                y_batch = y_batch.to(self.device)\n",
    "                pred,loss = self.model(x_batch,y_batch)\n",
    "                val_loss.append(loss.item())\n",
    "                val_smape.append(self.SMAPE(pred.reshape(-1),y_batch))\n",
    "            epoch_train_loss = sum(train_loss) / len(train_loss)\n",
    "            epoch_train_smape = sum(train_smape) / len(train_smape)\n",
    "            epoch_val_loss = sum(val_loss) / len(val_loss)\n",
    "            epoch_val_smape = sum(val_smape) / len(val_smape)\n",
    "            \n",
    "        if epoch_val_loss < self.min_loss:\n",
    "            self.min_loss = epoch_val_loss\n",
    "            self.best_model = self.model\n",
    "\n",
    "        return epoch_train_loss,epoch_val_loss,epoch_train_smape,epoch_val_smape\n",
    "    \n",
    "    \n",
    "    def train(self):\n",
    "        train_loss_list = []\n",
    "        val_loss_list = []\n",
    "        epoch_train_smape_list = []\n",
    "        epoch_val_smape_list = []\n",
    "        for i in range(self.epochs):\n",
    "            train_loss,val_loss,epoch_train_smape,epoch_val_smape = self.__train_one_peoch()\n",
    "            info_str = \"train_loss:%.4f val_loss:%.4f train_smape:%.4f val_smape:%.4f\\n\" % (train_loss,val_loss,epoch_train_smape,epoch_val_smape)\n",
    "            print(infor_str)\n",
    "            train_loss_list.append(train_loss)\n",
    "            val_loss_list.append(val_loss)\n",
    "            epoch_train_smape_list.append(epoch_train_smape)\n",
    "            epoch_val_smape_list.append(epoch_val_smape)\n",
    "            if i % 21 == 0:\n",
    "                self.save_model(i)\n",
    "        title = 'len=%d + %s + %s + %s + '%(window_len,self.best_model.name,self.best_model.loss_type,self.optim_type.name)\n",
    "        info_str = title + 'train_loss:%.4f val_loss:%.4f train_smape:%.4f val_smape:%.4f\\n'%(train_loss,val_loss,epoch_train_smape,epoch_val_smape)\n",
    "        logging.info(info_str)\n",
    "        draw(train_loss_list,val_loss_list,'epochs','loss',['train_loss','val_loss'],title)\n",
    "        draw(epoch_train_smape_list,epoch_val_smape_list,'epochs','smape',['train_smape','val_smape'],title)\n",
    "        \n",
    "        \n",
    "    def draw(self,y1,y2,xlabel,ylabel,lengend,title):\n",
    "        x = [i for i in range(len(y1))]\n",
    "        plt.plot(x,y1,x,y2)\n",
    "        plt.xlabel(xlabel)\n",
    "        plt.ylabel(ylabel)\n",
    "        plt.lengend(lengend)\n",
    "        plt.title(title)\n",
    "        plt.savefig(\"%s.png\"%title)\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    def eval(self,test_loader):\n",
    "        self.model.eval()\n",
    "        test_loss = []\n",
    "        with torch.no_grad():\n",
    "            # batch_size == total_num\n",
    "            for idx,(x_batch,y_batch) in enumerate(test_loader):\n",
    "                x_batch = x_batch.to(self.device)\n",
    "                y_batch = y_batch.to(self.device)\n",
    "                pred,loss = self.model(x_batch,y_label)\n",
    "                test_loss.append(loss.item())\n",
    "            test_loss = sum(test_loss) / len(test_loss)\n",
    "            smape_val = self.SMAPE(pred.reshape(-1),y_batch,Mode.Test)\n",
    "            \n",
    "        return test_loss,smape_val\n",
    "\n",
    "    \n",
    "    def SMAPE(self,pred,y_batch,mode=Mode.Trian):\n",
    "        pred = max_min_reverse(pred.cpu().numpy(),self.normalize_min_val,self.nmormalize_max_val)\n",
    "        if mode == Mode.Test:\n",
    "            y_batch = y_batch.cpu().numpy().reshape(-1)\n",
    "        else:\n",
    "            y_batch = max_min_reverse(y_batch.cpu().numpy(),self.normalize_min_val,self.nmormalize_max_val)\n",
    "        smape_val= smape(pred.reshape(-1),y_batch)\n",
    "        return smape_val\n",
    "        \n",
    "    def save_model(self,epoch=None):\n",
    "        if epoch is not None:\n",
    "            torch.save(self.best_model,'./checkpoint/len=%d + %s + %s + %s + checkpoint: %d.pt'\n",
    "                       %(window_len,self.best_model.name,self.best_model.loss_type,self.optim_type.name,epoch))\n",
    "        torch.save(self.best_model,'./checkpoint/len=%d + %s + %s + %s + test.pt'\n",
    "                       %(window_len,self.best_model.name,self.best_model.loss_type,self.optim_type.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "965dfc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = {\n",
    "    \"input_size\": window_len+output_size,\n",
    "    'output_size':output_size,\n",
    "    \"rnn_inpt_size\" : 128,\n",
    "    \"rnn_hidden_size\": 256,\n",
    "    \"loss_type\":LossFunction.MAE,\n",
    "    'dropout_rate':0.2\n",
    "}\n",
    "\n",
    "train_args = {\n",
    "    'lr' : 3e-4,\n",
    "    'batch_size' : 32,\n",
    "    'dropout_rate' : 0.2,\n",
    "    'optim' : Optim.SGD,\n",
    "    'epochs' : 300,\n",
    "}\n",
    "\n",
    "model = Seq2Seq(input_size=model_args[\"input_size\"],ouput_size=model_args['output_size'],\n",
    "                rnn_inpt_size=model_args['rnn_inpt_size'],rnn_hidden_size=model_args['rnn_hidden_size'],\n",
    "                loss_type=model_args['loss_type'],dropout_rate=model_args['dropout_rate'])\n",
    "\n",
    "trainer = Trainer(model,train_loader,val_loader,test_loader,min_val,max_val,train_args)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f41d0496",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                          | 0/101 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (32x56 and 112x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_26328/605387764.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_26328/1688152234.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[0mepoch_val_smape_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m             \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepoch_train_smape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepoch_val_smape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__train_one_peoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m             \u001b[0minfo_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"train_loss:%.4f val_loss:%.4f train_smape:%.4f val_smape:%.4f\\n\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepoch_train_smape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepoch_val_smape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfor_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_26328/1688152234.py\u001b[0m in \u001b[0;36m__train_one_peoch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[0mx_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[0my_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m             \u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\torch_cpu\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_26328/939439270.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, label)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m#         if mode == Mode.Test:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\torch_cpu\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_26328/1373959670.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0memb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# batch size * seq_len\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh_n\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn_input_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\torch_cpu\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\torch_cpu\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\torch_cpu\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1845\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1846\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1847\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1848\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (32x56 and 112x128)"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9526f01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
